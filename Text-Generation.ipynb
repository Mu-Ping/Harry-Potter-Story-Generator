{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text-Generation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMK4QLAp2x3+rawN3et12v/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UCPh61J6U7Np"},"source":["import nltk\n","import numpy as np\n","import keras\n","from keras.utils import to_categorical\n","from gensim.models.word2vec import Word2Vec\n","from tensorflow.keras import models\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Embedding, TimeDistributed, Dropout, GRU\n","from keras.optimizers import Adam, SGD\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n","nltk.download('punkt');\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6sxFhaIZ8uj"},"source":["\n","### 訓練資料前處理\n","\n","> **訓練資料斷詞**\n","\n","將五篇訓練資料讀入，並一句一句將其斷詞( tokenize )，此處使用nltk套件完成。\n","\n","> **Word2Vec訓練**\n","\n","將標籤化的句子使用gensim所提供的Word2Vec方法，來訓練詞向量\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"lbGHX-_JVCoC","executionInfo":{"status":"error","timestamp":1604602011771,"user_tz":-480,"elapsed":17358,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}},"outputId":"850e27c6-7153-4c74-9f66-38f7cb4c94d4","colab":{"base_uri":"https://localhost:8080/","height":376}},"source":["filename_list=[\"Harry-Potter-and-the-Chamber-of-Secrets.txt\",\n","        \"Harry-Potter-and-the-Deathly-Hallows.txt\",\n","        \"Harry-Potter-and-the-Goblet-of-Fire.txt\",\n","        \"Harry-Potter-and-the-Half-Blood-Prince.txt\",\n","        \"Harry-Potter-and-the-Order-of-the-Phoenix.txt\",\n","        \"Harry-Potter-and-the-Philosophers-Stone.txt\",\n","        \"Harry-Potter-and-the-Prisoner-of-Azkaban.txt\"]\n","word2vec=[]\n","\n","#為個別txt檔之word_tokenize\n","all_txt_word_tokenize=[] \n","\n","for file in filename_list:\n","  single_txt_word_tokenize=[]\n","  with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/HarryPotter-en/\"+file, 'r',encoding=\"utf-8\") as f:        \n","    for i in f.read().split(\"\\n\"):                    #建議使用\\n分割，整篇文章下去可能會出現錯誤，下方有範例 \n","      word_tokenize=nltk.word_tokenize(i)           #為甚麼不能用split(\"\")而用word_tokenize 解：https://reurl.cc/Q32y1p\n","      single_txt_word_tokenize.extend(word_tokenize)\n","      word2vec.append(word_tokenize) \n","  all_txt_word_tokenize.append(single_txt_word_tokenize)\n","  #word2vec 使用 Cosine Similarity 來計算兩個詞的相似性．這是一個 -1 到 1 的數值，如果兩個詞完全一樣就是 1\n","  # print(embedding_model.wv.most_similar('one')) #印出Cosine Similarity\n","\n","embedding_model = Word2Vec(word2vec, min_count=1, size=5)"],"execution_count":2,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-bacde6201794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/HarryPotter-en/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                    \u001b[0;31m#建議使用\\n分割，整篇文章下去可能會出現錯誤，下方有範例\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mword_tokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m#為甚麼不能用split(\"\")而用word_tokenize 解：https://reurl.cc/Q32y1p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0msingle_txt_word_tokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"WnKm_NHwcuFV"},"source":["\n","### 測試區\n","\n","> **「整段文章」 與 「單獨子句」 標籤化差異**\n","\n","PS：整段文章含\"\\n\"；單獨子句不含\n","\n","如果整段文章標籤化可能會出現錯誤，所以建議分成單獨子句來標籤化\n","\n"]},{"cell_type":"code","metadata":{"id":"eud8Cc6MB-dG","executionInfo":{"status":"aborted","timestamp":1604602011766,"user_tz":-480,"elapsed":17347,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["# 使用整篇文章tokenize；每句使用\\n分割再tokenize 差別比較\n","with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/HarryPotter-en/\"+filename_list[1], 'r',encoding=\"utf-8\") as f:\n","  word_tokenize_1=nltk.word_tokenize(f.read())\n","\n","single_txt_word_tokenize=[]\n","with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/HarryPotter-en/\"+filename_list[1], 'r',encoding=\"utf-8\") as f:\n","  for i in f.read().split(\"\\n\"): #為啥不能整個文章下去tokenize\n","    word_tokenize_2=nltk.word_tokenize(i)\n","    single_txt_word_tokenize.extend(word_tokenize_2)\n","\n","print(word_tokenize_1[670:675])\n","print(single_txt_word_tokenize[670:675])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YzSLyt0fd6I2"},"source":["### 轉換表生成\n","> **需要算出三種轉換表 index2word、word2index、index2vector**\n","  \n","index2word  -> [Word2vec_model].wv.index2word  \n","word2index  -> index2word 來算出  \n","index2vector -> [Word2vec_model].wv.vectors"]},{"cell_type":"code","metadata":{"id":"aP04vSl6VUs7","executionInfo":{"status":"aborted","timestamp":1604602011767,"user_tz":-480,"elapsed":17339,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["#需要算出三種轉換表 index2word、word2index、index2vector\n","# index2word [Word2vec_model].wv.index2word\n","# word2index 利用 index2word 來算出\n","# index2vector [Word2vec_model].wv.vectors\n","index2word=embedding_model.wv.index2word   \n","word2index= {}                            \n","index2vector=embedding_model.wv.vectors   \n","\n","for i in range(len(index2word)):\n","  word2index[index2word[i]]=i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-km6MYDGVZ9K","executionInfo":{"status":"aborted","timestamp":1604602011768,"user_tz":-480,"elapsed":17335,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["train_sentence_x=[]\n","train_sentence_y=[]\n","step=10 #設定步長\n","\n","#處理訓練資料------------將資料換成Word2Vec的Index 參考：https://reurl.cc/9XWMVd\n","for temp in all_txt_word_tokenize:\n","  x=temp[:-1]\n","  y=temp[1:]  \n","  for i in range(0,len(temp)-step, step):   #10步為一筆資料\n","    tempx=[]\n","    tempy=[]\n","    for j in range(step):\n","      tempx.append(word2index[x[i+j]]) #透過word2index轉換\n","      tempy.append(word2index[y[i+j]]) #透過word2index轉換\n","    train_sentence_x.append(tempx)\n","    train_sentence_y.append(tempy)\n","train_sentence_x=np.asarray(train_sentence_x)\n","train_sentence_y=np.asarray(train_sentence_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNINJDNJVdl6","executionInfo":{"status":"aborted","timestamp":1604602011768,"user_tz":-480,"elapsed":17331,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["allkind=[i for i in range(embedding_model.wv.vectors.shape[0])]\n","answer=to_categorical(allkind, embedding_model.wv.vectors.shape[0]) #預先存好one hot encoding\n","batch_size=512  #設定batch_size\n","\n","def train_sentence_generator():#預防資料過大\n","  while 1:\n","    tempx=[]\n","    tempy=[]\n","    for i in range(0, len(train_sentence_x)):\n","      temp=[]\n","      for j in train_sentence_y[i]:\n","        temp.append(answer[j])\n","      tempx.append(train_sentence_x[i])\n","      tempy.append(temp)  \n","      if((i+1)%batch_size==0):\n","        yield (np.asarray(tempx), np.asarray(tempy)) #這個元組（生成器的單個輸出）組成了單個的batch\n","        tempx=[]\n","        tempy=[]   \n","  #重要!!! keras的input都為[[data1],[data2]] 就算只有一個data 也要寫成[[data1]] 不可以是 [data]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4w5bJ3o8VfUq","executionInfo":{"status":"aborted","timestamp":1604602011769,"user_tz":-480,"elapsed":17330,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["model = Sequential()\n","model.add(Embedding(embedding_model.wv.vectors.shape[0], embedding_model.wv.vectors.shape[1], weights=[embedding_model.wv.vectors], input_length=10))\n","model.add(LSTM(1000, return_sequences=True))\n","model.add(Dense(embedding_model.wv.vectors.shape[0], activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_Kp0BTtViQx","executionInfo":{"status":"aborted","timestamp":1604602011770,"user_tz":-480,"elapsed":17329,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["#model = models.load_model('/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/model.h5')  #Adam(0.001)\n","\n","model.compile(loss=keras.losses.categorical_crossentropy, # 設定 Loss 損失函數\n","              optimizer=Adam(0.01),      # 設定 Optimizer 最佳化方法，此專案學習重要\n","              metrics=['accuracy'])\n","checkpoint = ModelCheckpoint(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/model.h5\", monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n","learning_rate_function = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=10, min_lr=0.00000001, mode='max')\n","model.fit_generator(train_sentence_generator(), steps_per_epoch =len(train_sentence_x)/batch_size, epochs=300, verbose=1, callbacks=[checkpoint, learning_rate_function]) #batch size不能太小：https://reurl.cc/14zQvV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdfthP-3hRWM","executionInfo":{"status":"aborted","timestamp":1604602011770,"user_tz":-480,"elapsed":17324,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["model = models.load_model('/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/model.h5')\n","txt=[\"story1.txt\", \"story2.txt\", \"story3.txt\"]\n","\n","for i in txt:\n","  temp_test_data=[]\n","  with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/\"+i, 'r',encoding=\"utf-8\") as f:\n","    text=f.read()\n","    word_tokenize=nltk.word_tokenize(text.replace(\"\\n\",\"\"))\n","    temp_test_data.extend(word_tokenize[2:]) #人名直接去除(此去除法並不精確)\n","\n","  test_data=[]\n","  #初始訓練參數，如果story初始未到10個字，補0\n","  for _ in range(10-len(temp_test_data)):\n","    test_data.append(0)\n","  #初始訓練參數，加入預設故事的前幾個字\n","  for j in temp_test_data:\n","    test_data.append(word2index[j])\n","\n","  #開始預測\n","  text=text.replace(\"\\n\",\"\")+\" \"\n","  for _ in range(500):\n","    nextword=model.predict_classes(np.asarray([test_data]))\n","    nextword=nextword[0][-1]\n","    test_data.pop(0) #刪除第一個字\n","    test_data.append(nextword)\n","    text=text + index2word[nextword]+\" \"\n","\n","  with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/106403546_\"+i, 'w') as f:\n","    f.write(text)"],"execution_count":null,"outputs":[]}]}