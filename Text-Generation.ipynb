{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text-Generation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMK4QLAp2x3+rawN3et12v/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UCPh61J6U7Np"},"source":["import nltk\n","import numpy as np\n","import keras\n","from keras.utils import to_categorical\n","from gensim.models.word2vec import Word2Vec\n","from tensorflow.keras import models\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Embedding, TimeDistributed, Dropout, GRU\n","from keras.optimizers import Adam, SGD\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n","nltk.download('punkt');\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6sxFhaIZ8uj"},"source":["\n","### 訓練資料前處理\n","\n","> **訓練資料斷詞**\n","\n","將五篇訓練資料讀入，並一句一句將其斷詞( tokenize )，此處使用nltk套件完成。\n","\n","> **Word2Vec訓練**\n","\n","將標籤化的句子使用gensim所提供的Word2Vec方法，來訓練詞向量\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"lbGHX-_JVCoC"},"source":["filename_list=[\"Harry-Potter-and-the-Chamber-of-Secrets.txt\",\n","        \"Harry-Potter-and-the-Deathly-Hallows.txt\",\n","        \"Harry-Potter-and-the-Goblet-of-Fire.txt\",\n","        \"Harry-Potter-and-the-Half-Blood-Prince.txt\",\n","        \"Harry-Potter-and-the-Order-of-the-Phoenix.txt\",\n","        \"Harry-Potter-and-the-Philosophers-Stone.txt\",\n","        \"Harry-Potter-and-the-Prisoner-of-Azkaban.txt\"]\n","word2vec=[]\n","\n","#為個別txt檔之word_tokenize\n","all_txt_word_tokenize=[] \n","\n","for file in filename_list:\n","  single_txt_word_tokenize=[]\n","  with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/HarryPotter-en/\"+file, 'r',encoding=\"utf-8\") as f:        \n","    for i in f.read().split(\"\\n\"):                    #建議使用\\n分割，整篇文章下去可能會出現錯誤，下方有範例 \n","      word_tokenize=nltk.word_tokenize(i)           #為甚麼不能用split(\"\")而用word_tokenize 解：https://reurl.cc/Q32y1p\n","      single_txt_word_tokenize.extend(word_tokenize)\n","      word2vec.append(word_tokenize) \n","  all_txt_word_tokenize.append(single_txt_word_tokenize)\n","  #word2vec 使用 Cosine Similarity 來計算兩個詞的相似性．這是一個 -1 到 1 的數值，如果兩個詞完全一樣就是 1\n","  # print(embedding_model.wv.most_similar('one')) #印出Cosine Similarity\n","\n","embedding_model = Word2Vec(word2vec, min_count=1, size=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WnKm_NHwcuFV"},"source":["\n","### 測試區\n","\n","> **「整段文章」 與 「單獨子句」 標籤化差異**\n","\n","PS：整段文章含\"\\n\"；單獨子句不含\n","\n","如果整段文章標籤化可能會出現錯誤，所以建議分成單獨子句來標籤化\n","\n"]},{"cell_type":"code","metadata":{"id":"eud8Cc6MB-dG","executionInfo":{"status":"aborted","timestamp":1604602011766,"user_tz":-480,"elapsed":17347,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["# 使用整篇文章tokenize；每句使用\\n分割再tokenize 差別比較\n","with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/HarryPotter-en/\"+filename_list[1], 'r',encoding=\"utf-8\") as f:\n","  word_tokenize_1=nltk.word_tokenize(f.read())\n","\n","single_txt_word_tokenize=[]\n","with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/HarryPotter-en/\"+filename_list[1], 'r',encoding=\"utf-8\") as f:\n","  for i in f.read().split(\"\\n\"): #為啥不能整個文章下去tokenize\n","    word_tokenize_2=nltk.word_tokenize(i)\n","    single_txt_word_tokenize.extend(word_tokenize_2)\n","\n","print(word_tokenize_1[670:675])\n","print(single_txt_word_tokenize[670:675])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YzSLyt0fd6I2"},"source":["### 轉換表生成\n","> **需要算出三種轉換表 index2word、word2index、index2vector**\n","  \n","index2word  -> [Word2vec_model].wv.index2word  \n","word2index  -> index2word 來算出  \n","index2vector -> [Word2vec_model].wv.vectors"]},{"cell_type":"code","metadata":{"id":"aP04vSl6VUs7","executionInfo":{"status":"aborted","timestamp":1604602011767,"user_tz":-480,"elapsed":17339,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["#需要算出三種轉換表 index2word、word2index、index2vector\n","# index2word [Word2vec_model].wv.index2word\n","# word2index 利用 index2word 來算出\n","# index2vector [Word2vec_model].wv.vectors\n","index2word=embedding_model.wv.index2word   \n","word2index= {}                            \n","index2vector=embedding_model.wv.vectors   \n","\n","for i in range(len(index2word)):\n","  word2index[index2word[i]]=i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-km6MYDGVZ9K","executionInfo":{"status":"aborted","timestamp":1604602011768,"user_tz":-480,"elapsed":17335,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["train_sentence_x=[]\n","train_sentence_y=[]\n","step=10 #設定步長\n","\n","#處理訓練資料------------將資料換成Word2Vec的Index 參考：https://reurl.cc/9XWMVd\n","for temp in all_txt_word_tokenize:\n","  x=temp[:-1]\n","  y=temp[1:]  \n","  for i in range(0,len(temp)-step, step):   #10步為一筆資料\n","    tempx=[]\n","    tempy=[]\n","    for j in range(step):\n","      tempx.append(word2index[x[i+j]]) #透過word2index轉換\n","      tempy.append(word2index[y[i+j]]) #透過word2index轉換\n","    train_sentence_x.append(tempx)\n","    train_sentence_y.append(tempy)\n","train_sentence_x=np.asarray(train_sentence_x)\n","train_sentence_y=np.asarray(train_sentence_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNINJDNJVdl6","executionInfo":{"status":"aborted","timestamp":1604602011768,"user_tz":-480,"elapsed":17331,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["allkind=[i for i in range(embedding_model.wv.vectors.shape[0])]\n","answer=to_categorical(allkind, embedding_model.wv.vectors.shape[0]) #預先存好one hot encoding\n","batch_size=512  #設定batch_size\n","\n","def train_sentence_generator():#預防資料過大\n","  while 1:\n","    tempx=[]\n","    tempy=[]\n","    for i in range(0, len(train_sentence_x)):\n","      temp=[]\n","      for j in train_sentence_y[i]:\n","        temp.append(answer[j])\n","      tempx.append(train_sentence_x[i])\n","      tempy.append(temp)  \n","      if((i+1)%batch_size==0):\n","        yield (np.asarray(tempx), np.asarray(tempy)) #這個元組（生成器的單個輸出）組成了單個的batch\n","        tempx=[]\n","        tempy=[]   \n","  #重要!!! keras的input都為[[data1],[data2]] 就算只有一個data 也要寫成[[data1]] 不可以是 [data]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4w5bJ3o8VfUq","executionInfo":{"status":"aborted","timestamp":1604602011769,"user_tz":-480,"elapsed":17330,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["model = Sequential()\n","model.add(Embedding(embedding_model.wv.vectors.shape[0], embedding_model.wv.vectors.shape[1], weights=[embedding_model.wv.vectors], input_length=10))\n","model.add(LSTM(1000, return_sequences=True))\n","model.add(Dense(embedding_model.wv.vectors.shape[0], activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_Kp0BTtViQx","executionInfo":{"status":"aborted","timestamp":1604602011770,"user_tz":-480,"elapsed":17329,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["#model = models.load_model('/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/model.h5')  #Adam(0.001)\n","\n","model.compile(loss=keras.losses.categorical_crossentropy, # 設定 Loss 損失函數\n","              optimizer=Adam(0.01),      # 設定 Optimizer 最佳化方法，此專案學習重要\n","              metrics=['accuracy'])\n","checkpoint = ModelCheckpoint(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/model.h5\", monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n","learning_rate_function = ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=10, min_lr=0.00000001, mode='max')\n","model.fit_generator(train_sentence_generator(), steps_per_epoch =len(train_sentence_x)/batch_size, epochs=300, verbose=1, callbacks=[checkpoint, learning_rate_function]) #batch size不能太小：https://reurl.cc/14zQvV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdfthP-3hRWM","executionInfo":{"status":"aborted","timestamp":1604602011770,"user_tz":-480,"elapsed":17324,"user":{"displayName":"陳信瑋","photoUrl":"","userId":"02199226763372812941"}}},"source":["model = models.load_model('/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/model.h5')\n","txt=[\"story1.txt\", \"story2.txt\", \"story3.txt\"]\n","\n","for i in txt:\n","  temp_test_data=[]\n","  with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/\"+i, 'r',encoding=\"utf-8\") as f:\n","    text=f.read()\n","    word_tokenize=nltk.word_tokenize(text.replace(\"\\n\",\"\"))\n","    temp_test_data.extend(word_tokenize[2:]) #人名直接去除(此去除法並不精確)\n","\n","  test_data=[]\n","  #初始訓練參數，如果story初始未到10個字，補0\n","  for _ in range(10-len(temp_test_data)):\n","    test_data.append(0)\n","  #初始訓練參數，加入預設故事的前幾個字\n","  for j in temp_test_data:\n","    test_data.append(word2index[j])\n","\n","  #開始預測\n","  text=text.replace(\"\\n\",\"\")+\" \"\n","  for _ in range(500):\n","    nextword=model.predict_classes(np.asarray([test_data]))\n","    nextword=nextword[0][-1]\n","    test_data.pop(0) #刪除第一個字\n","    test_data.append(nextword)\n","    text=text + index2word[nextword]+\" \"\n","\n","  with open(\"/content/drive/My Drive/Colab Notebooks/Ping/自然語言處理/106403546_\"+i, 'w') as f:\n","    f.write(text)"],"execution_count":null,"outputs":[]}]}